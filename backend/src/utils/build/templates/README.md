# Docker template files

We'll need to find a method that we can use to template sections of these Docker files.

Note that on either of these examples there is nothing wrong with just installing all python deps globally instead of using a conda-pack tar.

## Mlserver

[mlserver.dockerfile](mlserver.dockerfile)

This build file has 2 stages, in the first stage if it finds a conda env file in the docker context it will attempt to use that env file to build out the base conda env for the container then it will use conda-pack to create a self contained tar of the base environment.  The tar is extracted at startup.  This is nice but not necessary, if the second stage of the build finds a `requirements.txt` it will install the python dependencies into the final image.  This will save startup time but the overall image will be larger. MLServer does install all python dependencies globally so that might lead to conflicts.

Note that you can even have a mounted volume containing your `models` folder for MLServer models.  This can also contain the custom runtime with your conda-pack tar.

[MLServer custom python environment](https://mlserver.readthedocs.io/en/stable/examples/conda/README.html)

```yaml
MLSERVER_MODELS_DIR=/mnt/models
MLSERVER_ENV_TARBALL=/mnt/models/conda-pack.tar.gz
```

Really unless there are specific requirements we shouldn't need to make too many changes to use this as a template assuming it works well in Bailo's build pipeline.

This is based off the default docker file generated by mlserver with a couple of changes.

- On lines 9, 22 I've switched to having mamba resolve dependencies instead of conda.  In general we've found mamba far faster at resolving environment dependencies.

Note that any settings in MLServer's `settings.json` file can be overridden by environment variables. Normally you'll want to have at least the models folder set in an env. You don't need to rebuild the container each time you want to change a setting.

[MLSever server/model settings ref](https://mlserver.readthedocs.io/en/stable/reference/index.html#)

At startup MLServer will load all models at this base path. When debug settings are on you will see a "ModelA loaded" statement shortly after the container starts up.  If you don't see this likely it didn't find your models.  

```yaml
MLSERVER_MODELS_DIR=/mnt/models
```

### Folder structure for MLServer project

```txt
├── Dockerfile
├── Dockerfile.dockerignore
├── README.md
├── docker-compose.yml
├── requirements.txt (optional if using environment.yaml)
├── src
│   ├── environment.yaml (optional if using requirements.txt)
│   ├── models
│   │   ├── __init__.py
│   │   ├── modelA
│   │   │   ├── __init__.py
│   │   │   ├── model-settings.json
│   │   │   └── model.py
│   │   └── modelB
│   │       ├── __init__.py
│   │       ├── model-settings.json
│   │       └── model.py
│   └── settings.json (server settings)
```

## Triton

[triton.dockerfile](triton.dockerfile)

This is very similar to the template for MLServer (because I took their idea  and applied it to my Triton images).  Unlike the MLServer image though Triton's python backend will allow you to specify a conda-pack tar as an execution environment.  This is pretty nice as it allows each custom python backend to have an isolated environment.  

This build file has 2 stages, in the first stage if it finds a conda env file in the docker context it will attempt to use that env file to build out the base conda env for the container then it will use conda-pack to create a self contained tar of the base environment. The tar is extracted at startup.

The `config.pbtxt` file in the model folder defines how Triton will load this python backend.  You can specify a custom runtime in that config file with the following setting:

NOTE: `$$TRITON_MODEL_DIRECTORY` resolves to the current directory of this model.  If we follow the folder structure below that would be `$pwd/models/modelA`

```pbtxt
parameters: {
  key: "EXECUTION_ENV_PATH",
  value: {string_value: "$$TRITON_MODEL_DIRECTORY/model-conda.tar.gz"}
}
```

### Folder structure for Triton project

NOTE: I don't know why you need numbered folders under the model in triton... possibly for running different versions of the same python backend?  

```txt
├── README.md
├── docker-compose.yml
├── seldon_deployment.yaml
├── src
│   ├── Dockerfile
│   ├── environment.yml
│   └── models
│       └── modelA
│           ├── 1
│           │   └── model.py
│           └── config.pbtxt
            └── (After the build the conda-pack.tar will be here)
```

## Seldon s2i python wrapper

[seldonwrapper.dockerfile](seldonwrapper.dockerfile)

This is one you all are pretty familiar with already.  Just an example of what the dockerfile would look like if we were building the container in an environment without s2i