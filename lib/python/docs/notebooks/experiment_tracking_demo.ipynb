{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking with Bailo & MLFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to core concepts covered in previous notebooks, Bailo also offers integrations that might be useful within the wider machine learning lifecycle. This example notebook will run through **experiment tracking** in particular, integrating with **MLFlow Tracking**. The following concepts will be covered:\n",
    "\n",
    "* Creating a new experiment using a Bailo model.\n",
    "* Conducting experiment runs and logging parameters/metrics.\n",
    "* Importing existing experiments from **MLFlow Tracking**.\n",
    "* Publishing results to the Bailo service.\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "* Completion of the basic notebooks, in particular **models_and_releases_demo_pytorch.ipynb**.\n",
    "* Python 3.8.1 or higher (including a notebook environment for this demo).\n",
    "* A local or remote Bailo service (see https://github.com/gchq/Bailo).\n",
    "* Dependencies for MLFlow.\n",
    "* A local or remote MLFlow Tracking server, if following the MLFlow steps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Connecting with Bailo\n",
    "\n",
    "In order to create an `Experiment()` object, you'll first need to have a Bailo `Model()` object, and thus a defined `Client()` too. We learned how to do this in a previous notebook, but this time we'll create a new model with a custom schema which supports model metrics. *More on that later...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies...\n",
    "! pip install mlflow bailo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary import statements\n",
    "\n",
    "from bailo import Model, Client, Experiment, Schema, SchemaKind\n",
    "import mlflow\n",
    "import random\n",
    "\n",
    "# Instantiating the Bailo client\n",
    "\n",
    "client = Client(\"http://127.0.0.1:8080\") # <- INSERT BAILO URL (if not hosting locally)\n",
    "\n",
    "# Creating a demo model\n",
    "\n",
    "model = Model.create(client=client, name=\"YOLOv5\", description=\"YOLOv5 model for object detection.\", team_id=\"uncategorised\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up MLFlow Tracking\n",
    "\n",
    "In order to complete the integration element of this tutorial, we'll need to set up a local instance of MLFlow Tracking, and create a sample experiment run. *This will not contain any actual model training and is only to demonstrate the functionality of Bailo.*\n",
    "\n",
    "Run `mlflow ui` on the command line. Typically this will run on **localhost:5000** and the UI can be accessed on a browser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a custom schema for tracking\n",
    "\n",
    "The Bailo UI is designed to display metrics in a particular way, therefore you will need to use a schema that supports this. *This is necessary to publish results*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the schema in an external script as it is quite large!\n",
    "%run -i set_schema.py\n",
    "\n",
    "# Assigns a random schema ID\n",
    "schema_id = random.randint(1, 1000000)\n",
    "\n",
    "# Creates the schema on Bailo\n",
    "schema = Schema.create(client=client, schema_id=str(schema_id), name=\"Experiment Tracking\", description=\"Demo tracking schema\", kind=SchemaKind.MODEL, json_schema=json_schema)\n",
    "\n",
    "# Model cards need to be instantiated with their mandatory fields before metrics can be published.\n",
    "model.card_from_schema(schema_id=str(schema_id))\n",
    "new_card = {\n",
    "  'overview': {\n",
    "    'tags': [],\n",
    "    'modelSummary': 'YOLOv5 model for object detection.',\n",
    "  }\n",
    "}\n",
    "model.update_model_card(model_card=new_card)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new experiment\n",
    "\n",
    "Experiments with the Bailo client are created using the `Model.create_experiment()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = model.create_experiment()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting experiment runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an experiment with the Bailo python client\n",
    "\n",
    "You can run experiments directly using the Bailo python client as follows.\n",
    "\n",
    "**NOTE**: This will only work for sequential experiment runs, so if you're running experiments in parallel then it would be better to use **MLFlow Tracking**. We'll learn how to import completed experiments from MLFlow in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary params\n",
    "params = {\n",
    "    \"lr\": 0.01,\n",
    "    \"anchor_t\": 4.0,\n",
    "    \"scale\": 0.5,\n",
    "}\n",
    "\n",
    "# Arbitrary metrics\n",
    "metrics = {\n",
    "    \"accuracy\": 0.98,\n",
    "}\n",
    "\n",
    "for x in range(5):\n",
    "    experiment.start_run()\n",
    "    experiment.log_params(params)\n",
    "    experiment.log_metrics(metrics)\n",
    "    experiment.log_artifacts([\"weights.txt\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dummy MLFlow experiment run\n",
    "\n",
    "This section conducts an arbitrary experiment run and logs the params/metrics to your local MLFlow server. *We need this for the next section*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 13:42:29 INFO mlflow.tracking._tracking_service.client: 🏃 View run resilient-hound-42 at: http://127.0.0.1:5000/#/experiments/523964781059248151/runs/d97eae9e20bd4d82b6ad8c8b093fbefc.\n",
      "2024/08/20 13:42:29 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/523964781059248151.\n"
     ]
    }
   ],
   "source": [
    "# Setting local tracking URI and experiment name\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"hgfc\")\n",
    "\n",
    "# Logging the same metrics to the local MLFlow server\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"accuracy\", 0.86)\n",
    "    mlflow.log_artifact(\"weights.txt\")\n",
    "    mlflow.set_tag(\"Training Info\", \"YOLOv5 Demo Model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing existing experiments from MLFlow into Bailo\n",
    "\n",
    "As previously mentioned, you can import existing experiments into the `Experiment()` class by using the `Experiment.from_mlflow()` method. You must provide the **MLFlow tracking URI** and the experiment ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 117.77it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 372.53it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 168.81it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 94.45it/s] \n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 133.28it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 197.05it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 189.78it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 151.08it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 165.00it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 161.68it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 181.55it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 104.08it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 57.03it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 174.62it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment_mlflow = model.create_experiment()\n",
    "experiment_mlflow.from_mlflow(\n",
    "    tracking_uri=\"http://127.0.0.1:5000\", experiment_id=\"523964781059248151\"\n",
    ")  # <- INSERT MLFLOW EXPERIMENT ID. CAN BE FOUND ON THE UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publishing results to Bailo\n",
    "\n",
    "Experiment runs can be published to the model card using the `Experiment.publish()` method, **one at a time**. This is because the intended use is only to publish the most successful run.\n",
    "Therefore, you must specify the **run_id** to publish, or specify an order so the client can select the best result. As well as this, you must specify the location of the metrics in your schema (in this case *performance.performanceMetrics* as per the schema we defined earlier).\n",
    "\n",
    "Examples for both scenarios can be seen below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publishing a specific run\n",
    "\n",
    "To publish a specific run, you must pass the `run_id` into the method. In this example, use one of the IDs we created in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes seen\n",
      "sel_run = {'run': '5ce3285510264ff7b091c593e057bbf5', 'params': [{'lr': '0.01', 'anchor_t': '4.0', 'scale': '0.5'}], 'metrics': [{'accuracy': 0.86}], 'artifacts': ['/tmp/bailo_runs/mlflow_5ce3285510264ff7b091c593e057bbf5'], 'dataset': ''} of type: <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[34m          \u001b[0m| 0.00/238 [00:00<?, ?B/s, uploading mlflow_5ce3285510264ff7b091c593e057bbf5.zip]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m██████████\u001b[0m| 238/238 [00:00<00:00, 4.11kB/s, uploading mlflow_5ce3285510264ff7b091c593e057bbf5.zip]\n"
     ]
    }
   ],
   "source": [
    "experiment_mlflow.publish(mc_loc=\"performance.performanceMetrics\", run_id=\"5ce3285510264ff7b091c593e057bbf5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publishing the best run\n",
    "\n",
    "To publish the best run, you must define what the best is for your use case. This can be done using the `select_by` parameter with a string e.g. `accuracy MIN|MAX`. Depending on the requirements, `accuracy` could be any metric you have defined in your experiment.\n",
    "\n",
    "In the below example, we will use `accuracy MAX` to publish the experiment run with the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes seen\n",
      "sel_run = {'run': '48cb633ece3b4b83af3e26ffe6b42b26', 'params': [{'lr': '0.01', 'anchor_t': '4.0', 'scale': '0.5'}], 'metrics': [{'accuracy': 0.86}], 'artifacts': ['/tmp/bailo_runs/mlflow_48cb633ece3b4b83af3e26ffe6b42b26'], 'dataset': '', 'target': 0.86} of type: <class 'dict'>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/bailo_runs/mlflow_48cb633ece3b4b83af3e26ffe6b42b26'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment_mlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmc_loc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mperformance.performanceMetrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselect_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy MAX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/Bailo/lib/python/src/bailo/helper/model.py:582\u001b[0m, in \u001b[0;36mExperiment.publish\u001b[0;34m(self, mc_loc, semver, notes, run_id, select_by)\u001b[0m\n\u001b[1;32m    573\u001b[0m release_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcreate_release(version\u001b[38;5;241m=\u001b[39mrelease_new_version, minor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, notes\u001b[38;5;241m=\u001b[39mnotes)\n\u001b[1;32m    575\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading %d artifacts to version %s of model %s.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mlen\u001b[39m(artifacts),\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mstr\u001b[39m(release_new_version),\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m    580\u001b[0m )\n\u001b[0;32m--> 582\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m artifact \u001b[38;5;129;01min\u001b[39;00m artifacts:\n\u001b[1;32m    583\u001b[0m     release_new\u001b[38;5;241m.\u001b[39mupload(path\u001b[38;5;241m=\u001b[39martifact)\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_dir) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_dir):\n",
      "File \u001b[0;32m~/git/Bailo/lib/python/src/bailo/helper/release.py:250\u001b[0m, in \u001b[0;36mRelease.upload\u001b[0;34m(self, path, data)\u001b[0m\n\u001b[1;32m    247\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     name \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m--> 250\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_zip:\n\u001b[1;32m    253\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/bailo_runs/mlflow_48cb633ece3b4b83af3e26ffe6b42b26'"
     ]
    }
   ],
   "source": [
    "experiment_mlflow.publish(mc_loc=\"performance.performanceMetrics\", select_by=\"accuracy MAX\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful, our metrics should now be under the **Performance** tab of your model card on the UI! Additionally, our artifact will have been published as a new release (*this will have been done twice if you ran both the above steps*)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
